{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM3dLcFEstdKZGu1ig910nB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Timeseries 2 - Statistical Modeling (~15 minutes)\n","\n","## Layout:\n","\n","- (1) - Exponential Filtering (5 minutes)\n","- (2) - ARIMA (5 minutes)\n","- (3) - Prophet (5 minutes)"],"metadata":{"id":"rcLi0EvOUUyf"}},{"cell_type":"code","source":["### if you want to run on your own computer => upgrade required package\n","# ! pip install matplotlib --upgrade\n","# ! pip install pandas --upgrade\n","# ! pip install seaborn --upgrade\n","# ! pip install plotly --upgrade\n","# ! pip install pystan --upgrade\n","# ! pip install statsmodels\n","# ! pip install prophet --upgrade"],"metadata":{"id":"rZ57wfUhUvWr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"],"metadata":{"id":"j5yf0MYYU-nU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZJ9OongeuwQY"},"source":["## Loading data with read_csv:\n","\n","We do two specific things while loading:\n","\n","- `usecols`: We only consider the datetime and the count series\n","- `parse_dates` : We parse the datetime serie as dates\n","\n","NB: [read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html?highlight=read_csv#pandas.read_csv) has a TON of options, be sure to check them"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HGxHBdSIuwQY"},"outputs":[],"source":["#lets load the data and only consider the count as a serie.\n","df = pd.read_csv(\"https://raw.githubusercontent.com/vguigue/TimeSeries/main/data/train.csv\",parse_dates=[\"datetime\"],usecols=['datetime','count'])\n","df[\"minutes\"] = df.datetime.dt.minute\n","df[\"hour\"]  = df.datetime.dt.hour\n","df[\"day\"]  = df.datetime.dt.day\n","df[\"month\"]  = df.datetime.dt.month\n","df[\"year\"]  = df.datetime.dt.year\n","df[\"weekday\"]  = df.datetime.dt.day_of_week\n","\n","\n","time_indexed = df\n","time_indexed = time_indexed.set_index(\"datetime\")\n","time_indexed.head()\n"]},{"cell_type":"markdown","metadata":{"id":"wIV4zkp6uwQe"},"source":["# Simple forecasts : can we predict the two future days ?\n","\n","- Step 1: splitting the data before/after a given date\n","\n","- Step 2: use pandas embedded models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-O6rgn7VuwQe"},"outputs":[],"source":["month_data_train = time_indexed.loc[\"06-01-2011\":\"06-17-2011\"].copy()\n","month_data_test = time_indexed.loc[\"06-18-2011\":\"06-19-2011\"].copy()\n","\n","month_data_train[\"count\"].plot(figsize=(25,12))\n","month_data_test[\"count\"].plot(figsize=(25,12))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CkeeZVhiuwQf"},"outputs":[],"source":["month_data_train"]},{"cell_type":"markdown","metadata":{"id":"Lc6BnlSWuwQf"},"source":["Let's start with a naive hypothesis: \"tomorrow will be the same as today\". However, instead of a model like $\\hat{y}_{t} = y_{t-1}$ (which is actually a great baseline for any time series prediction problems and sometimes is impossible to beat), we will assume that the future value of our variable depends on the average of its $k$ previous values. Therefore, we will use the **moving average**.\n","\n","$\\hat{y}_{t} = \\frac{1}{k} \\displaystyle\\sum^{k}_{n=1} y_{t-n}$\n","\n","Unfortunately, we cannot make predictions far in the future - in order to get the value for the next step, we need the previous values to be actually observed. But moving average has another use case - smoothing the original time series to identify trends. Pandas has an implementation available with [`DataFrame.rolling(window).mean()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html). The wider the window, the smoother the trend. In the case of very noisy data, which is often encountered in finance, this procedure can help detect common patterns.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iJRAFBgWuwQf"},"outputs":[],"source":["month_data_train[\"ma_2h\"] = month_data_train[\"count\"].rolling(window=2).mean()\n","month_data_train[\"ma_6h\"] = month_data_train[\"count\"].rolling(window=6).mean()\n","\n","month_data_train[[\"count\",\"ma_2h\",\"ma_6h\"]].plot(figsize=(25,12))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Aff7mW-huwQf"},"source":["### => as it can be seen on the graph, it's hard to shift the mean forward to predict accurately."]},{"cell_type":"markdown","metadata":{"id":"Hj1tqss6uwQf"},"source":["## Exponential smoothing"]},{"cell_type":"markdown","metadata":{"id":"4zc39FaBuwQf"},"source":["Now, let's see what happens if, instead of weighting the last $k$ values of the time series, we start weighting all available observations while exponentially decreasing the weights as we move further back in time. There exists a formula for **[exponential smoothing](https://en.wikipedia.org/wiki/Exponential_smoothing)** that will help us with this:\n","\n","$$\\hat{y}_{t} = \\alpha \\cdot y_t + (1-\\alpha) \\cdot \\hat y_{t-1} $$\n","\n","Here the model value is a weighted average between the current true value and the previous model values. The $\\alpha$ weight is called a smoothing factor. It defines how quickly we will \"forget\" the last available true observation. The smaller $\\alpha$ is, the more influence the previous observations have and the smoother the series is.\n","\n","Exponentiality is hidden in the recursiveness of the function – we multiply by $(1-\\alpha)$ each time, which already contains a multiplication by $(1-\\alpha)$ of previous model values"]},{"cell_type":"markdown","metadata":{"id":"wHyvjmfZuwQf"},"source":["This is also [built in pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KPnB3XDPuwQf"},"outputs":[],"source":["\n","month_data_train[\"ewm_05\"] = month_data_train[\"count\"].ewm(alpha=0.5,adjust=False).mean()\n","month_data_train[\"ewm_01\"] = month_data_train[\"count\"].ewm(alpha=0.1,adjust=False).mean()\n","\n","month_data_train[[\"count\",\"ewm_05\",\"ewm_01\"]].plot(figsize=(25,12))"]},{"cell_type":"markdown","metadata":{"id":"Hx-1qRXHuwQg"},"source":["# Statsmodels -- Straightforward series analysis & Forecasting\n","\n","## Statsmodels\n","\n","### First, you can do the same things as with pandas: like exp. smoothing:"]},{"cell_type":"markdown","metadata":{"id":"59nFFAvRuwQg"},"source":["#### (a) Setting a DatetimeIndex Frequency\n","Note that our DatetimeIndex does not have a frequency. In order to build a Holt-Winters smoothing model, statsmodels needs to know the frequency of the data (whether it's daily, monthly etc.). Since observations occur each hour, we'll use H.<br>A full list of time series offset aliases can be found <a href='http://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases'>here</a>."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QcK0wFO2uwQg"},"outputs":[],"source":["month_data_train.index.freq = \"H\"\n","month_data_test.index.freq = \"H\"\n","month_data_train.index\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lTvkUQ_VuwQg"},"source":["Simple exp. smoothing yields the same values:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T-PXkKcnuwQg"},"outputs":[],"source":["from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n","\n","#For some reason, when optimized=False is passed into .fit()\n","#the statsmodels SimpleExpSmoothing function shifts fitted values down one row.\n","#We fix this by adding .shift(-1) after .fittedvalues\n","\n","month_data_train['sm_ewm_05']  = SimpleExpSmoothing(month_data_train[\"count\"]).fit(smoothing_level=0.5,optimized=False).fittedvalues.shift(-1)\n","month_data_train['sm_ewm_01']  = SimpleExpSmoothing(month_data_train[\"count\"]).fit(smoothing_level=0.1,optimized=False).fittedvalues.shift(-1)\n","\n","month_data_train[[\"count\",\"sm_ewm_05\",\"sm_ewm_01\"]].plot(figsize=(25,12))\n","\n","month_data_train[[\"ewm_05\",\"sm_ewm_05\",\"ewm_01\",\"sm_ewm_01\"]].head()"]},{"cell_type":"markdown","metadata":{"id":"DLrogSVLuwQg"},"source":["## Forecasting with `.forecast`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OgKjRN71uwQh"},"outputs":[],"source":["model = SimpleExpSmoothing(month_data_train[\"count\"]).fit(smoothing_level=0.5,optimized=False)\n","\n","month_data_test[\"ewm_05\"] = model.forecast(48) # we forecast on 2 days\n","\n","month_data_test[[\"count\",\"ewm_05\"]].plot(figsize=(25,12))\n"]},{"cell_type":"markdown","metadata":{"id":"ZDrhNkV4uwQh"},"source":["### Holt-Winters Methods\n","In the previous cells  we applied <em>Simple Exponential Smoothing</em> using just one smoothing factor $\\alpha$ (alpha). This failed to account for other contributing factors like trend and seasonality.\n","\n","In this section we'll look at <em>Double</em> and <em>Triple Exponential Smoothing</em> with the <a href='https://otexts.com/fpp2/holt-winters.html'>Holt-Winters Methods</a>.\n","\n","In <strong>Double Exponential Smoothing</strong> (aka Holt's Method) we introduce a new smoothing factor $\\beta$ (beta) that addresses trend:\n","\n","\\begin{split}l_t &= (1 - \\alpha) l_{t-1} + \\alpha x_t, & \\text{    level}\\\\\n","b_t &= (1-\\beta)b_{t-1} + \\beta(l_t-l_{t-1}) & \\text{    trend}\\\\\n","y_t &= l_t + b_t & \\text{    fitted model}\\\\\n","\\hat y_{t+h} &= l_t + hb_t & \\text{    forecasting model (} h = \\text{# periods into the future)}\\end{split}\n","\n","Because we haven't yet considered seasonal fluctuations, the forecasting model is simply a straight sloped line extending from the most recent data point. We'll see an example of this in upcoming lectures.\n","\n","With <strong>Triple Exponential Smoothing</strong> (aka the Holt-Winters Method) we introduce a smoothing factor $\\gamma$ (gamma) that addresses seasonality:\n","\n","\\begin{split}l_t &= (1 - \\alpha) l_{t-1} + \\alpha x_t, & \\text{    level}\\\\\n","b_t &= (1-\\beta)b_{t-1} + \\beta(l_t-l_{t-1}) & \\text{    trend}\\\\\n","c_t &= (1-\\gamma)c_{t-L} + \\gamma(x_t-l_{t-1}-b_{t-1}) & \\text{    seasonal}\\\\\n","y_t &= (l_t + b_t) c_t & \\text{    fitted model}\\\\\n","\\hat y_{t+m} &= (l_t + mb_t)c_{t-L+1+(m-1)modL} & \\text{    forecasting model (} m = \\text{# periods into the future)}\\end{split}\n","\n","Here $L$ represents the number of divisions per cycle. In our case looking at monthly data that displays a repeating pattern each year, we would use $L=12$.\n","\n","In general, higher values for $\\alpha$, $\\beta$ and $\\gamma$ (values closer to 1), place more emphasis on recent data.\n","\n","<h3>Related Functions:</h3>\n","<tt><strong><a href='https://www.statsmodels.org/stable/generated/statsmodels.tsa.holtwinters.SimpleExpSmoothing.html'>statsmodels.tsa.holtwinters.SimpleExpSmoothing</a></strong><font color=black>(endog)</font>&nbsp;&nbsp;&nbsp;&nbsp;\n","Simple Exponential Smoothing<br>\n","<strong><a href='https://www.statsmodels.org/stable/generated/statsmodels.tsa.holtwinters.ExponentialSmoothing.html'>statsmodels.tsa.holtwinters.ExponentialSmoothing</a></strong><font color=black>(endog)</font>&nbsp;&nbsp;\n","    Holt-Winters Exponential Smoothing</tt>\n","    \n","<h3>For Further Reading:</h3>\n","<tt>\n","<strong>\n","<a href='https://www.itl.nist.gov/div898/handbook/pmc/section4/pmc43.htm'>NIST/SEMATECH e-Handbook of Statistical Methods</a></strong>&nbsp;&nbsp;<font color=black>What is Exponential Smoothing?</font></tt>"]},{"cell_type":"markdown","metadata":{"id":"PcgejmMhuwQh"},"source":["___\n","## Double Exponential Smoothing\n","Where Simple Exponential Smoothing employs just one smoothing factor $\\alpha$ (alpha), Double Exponential Smoothing adds a second smoothing factor $\\beta$ (beta) that addresses trends in the data. Like the alpha factor, values for the beta factor fall between zero and one ($0<\\beta≤1$). The benefit here is that the model can anticipate future increases or decreases where the level model would only work from recent calculations.\n","\n","We can also address different types of change (growth/decay) in the trend. If a time series displays a straight-line sloped trend, you would use an <strong>additive</strong> adjustment. If the time series displays an exponential (curved) trend, you would use a <strong>multiplicative</strong> adjustment.\n","\n","As we move toward forecasting, it's worth noting that both additive and multiplicative adjustments may become exaggerated over time, and require <em>damping</em> that reduces the size of the trend over future periods until it reaches a flat line."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qo3R1ICkuwQh"},"outputs":[],"source":["from statsmodels.tsa.holtwinters import ExponentialSmoothing\n","\n","#additive\n","month_data_train['doubleEs_add'] =ExponentialSmoothing(month_data_train[\"count\"], trend='add').fit().fittedvalues.shift(-1)\n","\n","#multiplicative\n","month_data_train['doubleEs_mul'] =ExponentialSmoothing(month_data_train[\"count\"], trend='mul').fit().fittedvalues.shift(-1)\n","\n","month_data_train[[\"count\",\"doubleEs_add\",\"doubleEs_mul\"]].plot(figsize=(25,12))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFahbeKMuwQi"},"outputs":[],"source":["model2 = ExponentialSmoothing(month_data_train[\"count\"], trend='add').fit()\n","model2mul = ExponentialSmoothing(month_data_train[\"count\"], trend='mul').fit()\n","\n","month_data_test[\"doubleEs_add\"] = model2.forecast(48) # we forecast on 2 days\n","month_data_test[\"doubleEs_mul\"] = model2mul.forecast(48) # we forecast on 2 days\n","\n","month_data_test[[\"count\",\"doubleEs_add\",\"doubleEs_mul\"]].plot(figsize=(25,12))"]},{"cell_type":"markdown","metadata":{"id":"TS77C-W5uwQi"},"source":["## Triple Exponential Smoothing\n","Triple Exponential Smoothing, the method most closely associated with Holt-Winters, adds support for both trends and seasonality in the data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ipv3lm0uwQi"},"outputs":[],"source":["from statsmodels.tsa.holtwinters import ExponentialSmoothing\n","\n","month_data_train['tripleEs_add'] =ExponentialSmoothing(month_data_train[\"count\"], trend='add',seasonal='add',seasonal_periods=24).fit().fittedvalues.shift(-1)\n","\n","month_data_train[[\"count\",\"doubleEs_add\",\"doubleEs_mul\",\"tripleEs_add\"]].plot(figsize=(25,12))\n"]},{"cell_type":"markdown","metadata":{"id":"OV5LqRoeuwQi"},"source":["## Forecast with triple exponential smoothing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NbZXIpC4uwQi"},"outputs":[],"source":["model3 = ExponentialSmoothing(month_data_train[\"count\"], trend='mul',seasonal='add',seasonal_periods=24).fit()\n","\n","month_data_test[\"tripleEs_add\"] = model3.forecast(48)\n","\n","month_data_test[[\"count\",\"tripleEs_add\"]].plot(figsize=(25,12))"]},{"cell_type":"markdown","metadata":{"id":"MLab2BvOuwQi"},"source":["## There are many models to model time series as \"moving averages\":"]},{"cell_type":"markdown","metadata":{"id":"BIOLmb9PuwQi"},"source":["### [Be sure to check statsmodel docs](https://www.statsmodels.org/stable/index.html)"]},{"cell_type":"markdown","metadata":{"id":"JfTyHMBRWBxb"},"source":["# Econometric approach"]},{"cell_type":"markdown","metadata":{"id":"6S-KHXqcWBxb"},"source":["## Getting rid of non-stationarity and building SARIMA"]},{"cell_type":"markdown","metadata":{"code_folding":[],"id":"QuvlRi7TWBxb"},"source":["Let's build an ARIMA model."]},{"cell_type":"markdown","metadata":{"id":"3ZnYx_aUWBxc"},"source":["Here is the code to render plots."]},{"cell_type":"code","source":["import seaborn as sns                            # more plots\n","sns.set()\n","\n","from dateutil.relativedelta import relativedelta # working with dates with style\n","from scipy.optimize import minimize              # for function minimization\n","\n","import statsmodels.formula.api as smf            # statistics and econometrics\n","import statsmodels.tsa.api as smt\n","import statsmodels.api as sm\n","import scipy.stats as scs\n","\n","from itertools import product                    # some useful functions\n","from tqdm import tqdm_notebook\n","\n","import warnings                                  # `do not disturbe` mode\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"mLHMaod2YKwM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"code_folding":[],"id":"Ri3M9tAsWBxc"},"outputs":[],"source":["def tsplot(y, lags=None, figsize=(12, 7), style='bmh'):\n","    \"\"\"\n","        Plot time series, its ACF and PACF, calculate Dickey–Fuller test\n","\n","        y - timeseries\n","        lags - how many lags to include in ACF, PACF calculation\n","    \"\"\"\n","    if not isinstance(y, pd.Series):\n","        y = pd.Series(y)\n","\n","    with plt.style.context(style):\n","        fig = plt.figure(figsize=figsize)\n","        layout = (2, 2)\n","        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n","        acf_ax = plt.subplot2grid(layout, (1, 0))\n","        pacf_ax = plt.subplot2grid(layout, (1, 1))\n","\n","        y.plot(ax=ts_ax)\n","        p_value = sm.tsa.stattools.adfuller(y)[1]\n","        ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n","        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n","        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n","        plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5uo_ZJacWBxd"},"outputs":[],"source":["tsplot(month_data_train[\"count\"], lags=60)"]},{"cell_type":"markdown","metadata":{"id":"UeVTDfSsWBxd"},"source":["_this outlier on partial autocorrelation plot looks like a statsmodels bug, partial autocorrelation shall be <= 1 like any correlation._\n","\n","Surprisingly, the initial series are stationary; the Dickey-Fuller test rejected the null hypothesis that a unit root is present. Actually, we can see this on the plot itself – we do not have a visible trend, so the mean is constant and the variance is pretty much stable. The only thing left is seasonality, which we have to deal with prior to modeling. To do so, let's take the \"seasonal difference\", which means a simple subtraction of the series from itself with a lag that equals the seasonal period."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pGIV1WOfWBxd"},"outputs":[],"source":["data_diff = month_data_train[\"count\"] - month_data_train[\"count\"].shift(24)\n","tsplot(data_diff[24:], lags=60)"]},{"cell_type":"markdown","metadata":{"id":"z9nZcFT-WBxd"},"source":["It is now much better with the visible seasonality gone. However, the autocorrelation function still has too many significant lags. To remove them, we'll take first differences, subtracting the series from itself with lag 1."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dhEJuOrBWBxd"},"outputs":[],"source":["data_diff = data_diff - data_diff.shift(1)\n","tsplot(data_diff[24+1:], lags=60)"]},{"cell_type":"markdown","metadata":{"id":"v6R6hHVOWBxd"},"source":["Perfect! Our series now looks like something undescribable, oscillating around zero. The Dickey-Fuller test indicates that it is stationary, and the number of significant peaks in ACF has dropped. We can finally start modeling!"]},{"cell_type":"markdown","metadata":{"id":"x_NOApt4WBxd"},"source":["## ARIMA-family Crash-Course\n","\n","We will explain this model by building up letter by letter. $SARIMA(p, d, q)(P, D, Q, s)$, Seasonal Autoregression Moving Average model:\n","\n","- $AR(p)$ - autoregression model i.e. regression of the time series onto itself. The basic assumption is that the current series values depend on its previous values with some lag (or several lags). The maximum lag in the model is referred to as $p$. To determine the initial $p$, you need to look at the PACF plot and find the biggest significant lag after which **most** other lags become insignificant.\n","- $MA(q)$ - moving average model. Without going into too much detail, this models the error of the time series, again with the assumption that the current error depends on the previous with some lag, which is referred to as $q$. The initial value can be found on the ACF plot with the same logic as before.\n","\n","Let's combine our first 4 letters:\n","\n","$AR(p) + MA(q) = ARMA(p, q)$\n","\n","What we have here is the Autoregressive–moving-average model! If the series is stationary, it can be approximated with these 4 letters. Let's continue.\n","\n","- $I(d)$ - order of integration. This is simply the number of nonseasonal differences needed to make the series stationary. In our case, it's just 1 because we used first differences.\n","\n","Adding this letter to the four gives us the $ARIMA$ model which can handle non-stationary data with the help of nonseasonal differences. Great, one more letter to go!\n","\n","- $S(s)$ - this is responsible for seasonality and equals the season period length of the series\n","\n","With this, we have three parameters: $(P, D, Q)$\n","\n","- $P$ - order of autoregression for the seasonal component of the model, which can be derived from PACF. But you need to look at the number of significant lags, which are the multiples of the season period length. For example, if the period equals 24 and we see the 24-th and 48-th lags are significant in the PACF, that means the initial $P$ should be 2.\n","\n","- $Q$ - similar logic using the ACF plot instead.\n","\n","- $D$ - order of seasonal integration. This can be equal to 1 or 0, depending on whether seasonal differeces were applied or not."]},{"cell_type":"markdown","metadata":{"id":"i7fWcX_zWBxd"},"source":["Now that we know how to set the initial parameters, let's have a look at the final plot once again and set the parameters:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pE9Z4JZPWBxe"},"outputs":[],"source":["tsplot(data_diff[24+1:], lags=60)"]},{"cell_type":"markdown","metadata":{"id":"JhF8JUpPWBxe"},"source":["- $p$ is most probably 4 since it is the last significant lag on the PACF, after which, most others are not significant.\n","- $d$ equals 1 because we had first differences\n","- $q$ should be somewhere around 4 as well as seen on the ACF\n","- $P$ might be 2, since 24-th and 48-th lags are somewhat significant on the PACF\n","- $D$ again equals 1 because we performed seasonal differentiation\n","- $Q$ is probably 1. The 24-th lag on ACF is significant while the 48-th is not."]},{"cell_type":"markdown","metadata":{"id":"bjy_BdtTWBxe"},"source":["Let's test various models and see which one is better."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b1tev8quWBxe"},"outputs":[],"source":["# grid searching => range can be reduced to save time in the practical session\n","# setting initial values and some bounds for them\n","ps = range(3, 5) # (2,5)\n","d=1\n","qs = range(3, 5) # (2,5)\n","Ps = range(1, 2) # (0,2)\n","D=1\n","Qs = range(1, 2) # (0,2)\n","s = 24 # season length is still 24\n","\n","# creating list with all the possible combinations of parameters\n","parameters = product(ps, qs, Ps, Qs)\n","parameters_list = list(parameters)\n","len(parameters_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u3cA5k5yWBxe"},"outputs":[],"source":["def optimizeSARIMA(parameters_list, d, D, s):\n","    \"\"\"\n","        Return dataframe with parameters and corresponding AIC\n","\n","        parameters_list - list with (p, q, P, Q) tuples\n","        d - integration order in ARIMA model\n","        D - seasonal integration order\n","        s - length of season\n","    \"\"\"\n","\n","    results = []\n","    best_aic = float(\"inf\")\n","\n","    for param in tqdm_notebook(parameters_list):\n","        # we need try-except because on some combinations model fails to converge\n","        try:\n","            model=sm.tsa.statespace.SARIMAX(month_data_train[\"count\"], order=(param[0], d, param[1]),\n","                                            seasonal_order=(param[2], D, param[3], s)).fit(disp=-1)\n","        except:\n","            continue\n","        aic = model.aic\n","        # saving best model, AIC and parameters\n","        if aic < best_aic:\n","            best_model = model\n","            best_aic = aic\n","            best_param = param\n","        results.append([param, model.aic])\n","\n","    result_table = pd.DataFrame(results)\n","    result_table.columns = ['parameters', 'aic']\n","    # sorting in ascending order, the lower AIC is - the better\n","    result_table = result_table.sort_values(by='aic', ascending=True).reset_index(drop=True)\n","\n","    return result_table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0pvvfIY5WBxe"},"outputs":[],"source":["%%time\n","#best are supposed to be : (2, 4, 1, 1)\n","result_table = optimizeSARIMA(parameters_list, d, D, s)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D1HBgvLyWBxe"},"outputs":[],"source":["result_table.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ZdP1Ej6WBxe"},"outputs":[],"source":["# set the parameters that give the lowest AIC\n","# p, q, P, Q = result_table.parameters[0] # to save computation time\n","p, q, P, Q = (2, 4, 1, 1)\n","\n","best_model=sm.tsa.statespace.SARIMAX(month_data_train[\"count\"], order=(p, d, q),\n","                                        seasonal_order=(P, D, Q, s)).fit(disp=-1)\n","print(best_model.summary())"]},{"cell_type":"code","source":["def mean_absolute_percentage_error(y_true, y_pred):\n","    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","def MSE(y_true, y_pred):\n","    return np.mean((y_true - y_pred)**2)"],"metadata":{"id":"OHKa2_mJbeOA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"code_folding":[0],"id":"nlfctxjOWBxe"},"outputs":[],"source":["def plotSARIMA(series, model, n_steps):\n","    \"\"\"\n","        Plots model vs predicted values\n","\n","        series - dataset with timeseries\n","        model - fitted SARIMA model\n","        n_steps - number of steps to predict in the future\n","\n","    \"\"\"\n","    # adding model values\n","    data = pd.DataFrame(data=series, columns=['actual'])\n","    data['arima_model'] = model.fittedvalues\n","    # making a shift on s+d steps, because these values were unobserved by the model\n","    # due to the differentiating\n","    data['arima_model'][:s+d] = np.NaN\n","\n","    # forecasting on n_steps forward\n","    forecast = model.predict(start = data.shape[0], end = data.shape[0]+n_steps)\n","    forecast = data.arima_model.append(forecast)\n","    # calculate error, again having shifted on s+d steps from the beginning\n","    error = mean_absolute_percentage_error(data['actual'][s+d:], data['arima_model'][s+d:])\n","\n","    plt.figure(figsize=(15, 7))\n","    plt.title(\"Mean Absolute Percentage Error: {0:.2f}%\".format(error))\n","    plt.plot(forecast, color='r', label=\"model\")\n","    plt.axvspan(data.index[-1], forecast.index[-1], alpha=0.5, color='lightgrey')\n","    plt.plot(data.actual, label=\"actual\")\n","    plt.legend()\n","    plt.grid(True);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NesdU0eVWBxf"},"outputs":[],"source":["plotSARIMA(month_data_train[\"count\"], best_model, 30)"]},{"cell_type":"markdown","metadata":{"id":"Ddrs2MrWWBxf"},"source":["In the end, we got very adequate predictions. Our model was wrong by 4.01% on average, which is very, very good. However, the overall costs of preparing data, making the series stationary, and selecting parameters might not be worth this accuracy."]},{"cell_type":"markdown","metadata":{"id":"heJ4butJWBxf"},"source":["# Linear (and not quite) models for time series\n","\n","Often, in my job, I have to build models with [*fast, good, cheap*](http://fastgood.cheap) as my only guiding principle. That means that some of these models will never be considered \"production ready\" as they demand too much time for data preparation (as in SARIMA) or require frequent re-training on new data (again, SARIMA) or are difficult to tune (good example - SARIMA). Therefore, it's very often much easier to select a few features from the existing time series and build a simple linear regression model or, say, a random forest. It is good and cheap.\n","\n","This approach is not backed by theory and breaks several assumptions (e.g. Gauss-Markov theorem, especially for errors being uncorrelated), but it is very useful in practice and is often used in machine learning competitions.\n"]},{"cell_type":"markdown","metadata":{"id":"WRBtYwGKuwQi"},"source":["# (3)  Prophet\n","\n","> Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.\n","\n","\n","- **Accurate and fast** : Prophet is used in many applications across Facebook for producing reliable forecasts for planning and goal setting. We’ve found it to perform better than any other approach in the majority of cases. We fit models in Stan so that you get forecasts in just a few seconds.\n","\n","- **Fully automatic** : Get a reasonable forecast on messy data with no manual effort. Prophet is robust to outliers, missing data, and dramatic changes in your time series.\n","\n","- **Tunable forecasts** : The Prophet procedure includes many possibilities for users to tweak and adjust forecasts. You can use human-interpretable parameters to improve your forecast by adding your domain knowledge.\n","\n","- **Available in R or Python** : We’ve implemented the Prophet procedure in R and Python, but they share the same underlying Stan code for fitting. Use whatever language you’re comfortable with to get forecasts.\n","\n","\n","**You should really read the paper for Prophet! It is relatively straightforward and has a lot of insight on their techniques on how Prophet works internally!**\n","\n","Link to paper: https://peerj.com/preprints/3190.pdf"]},{"cell_type":"code","source":["! pip install prophet"],"metadata":{"id":"G7P70ALT8PIc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1MLTXe6NuwQi"},"outputs":[],"source":["from prophet import Prophet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_hTwHMqJuwQj"},"outputs":[],"source":["month_data_train.head()"]},{"cell_type":"markdown","metadata":{"id":"nu3xS5LEuwQj"},"source":["### Formatting data for Prophet:\n","\n","> The input to Prophet is always a dataframe with two columns: ds and y. The ds (datestamp) column should be of a format expected by Pandas, ideally YYYY-MM-DD for a date or YYYY-MM-DD HH:MM:SS for a timestamp. The y column must be numeric, and represents the measurement we wish to forecast."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7XyTUnmjuwQj"},"outputs":[],"source":["prophet_data_train = month_data_train[\"count\"].reset_index().rename({'datetime':'ds', 'count':'y'}, axis='columns').copy()\n","prophet_data_train.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"490gBJ-GuwQj"},"outputs":[],"source":["prophet_data_test = month_data_test[\"count\"].reset_index().rename({'datetime':'ds', 'count':'y'}, axis='columns').copy()\n","prophet_data_test.head()"]},{"cell_type":"markdown","metadata":{"id":"4BLVdOQZuwQj"},"source":["### Using Prophet\n","\n","Prophet follows the sklearn model API. We create an instance of the Prophet class and then call its fit and predict methods.\n","\n","**NOTE: Prophet by default is for daily data. You need to pass a frequency for sub-daily or monthly data. Info: https://facebook.github.io/prophet/docs/non-daily_data.html**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BhOlq1t9uwQj"},"outputs":[],"source":["mod = Prophet()\n","mod.fit(prophet_data_train)\n","future = mod.make_future_dataframe(periods=48, freq='H')\n","forecast = mod.predict(future)"]},{"cell_type":"markdown","metadata":{"id":"n-cCRfsTuwQj"},"source":["## Let's see how the forecast performed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vGe32PkpuwQj"},"outputs":[],"source":["predicted_days = forecast[-48:].set_index(\"ds\").sort_index().copy()\n","predicted_days[\"ground_truth\"] = month_data_test.sort_index()[\"count\"].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D311YiHwuwQj"},"outputs":[],"source":["predicted_days.head()"]},{"cell_type":"markdown","metadata":{"id":"rGNvTJ6tuwQj"},"source":["### plot the prediction:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NB6ui5GiuwQj"},"outputs":[],"source":["predicted_days[[\"yhat\",\"ground_truth\"]].reset_index(drop=True).plot(figsize=(25,12)) # for some reason, the index messes things up\n","# predicted_days[\"ground_truth\"].reset_index(drop=True).plot(figsize=(25,12))"]},{"cell_type":"markdown","metadata":{"id":"zF4BhCU1uwQk"},"source":["### => With prophet, you can easily see learnt componants."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"onrsK60_uwQk"},"outputs":[],"source":["mod.plot_components(forecast)"]},{"cell_type":"markdown","metadata":{"id":"vgBNgoJKuwQk"},"source":["## [If you have time left, have a look at prophet docs !](https://facebook.github.io/prophet/docs/quick_start.html#python-api)\n","\n","#### => Especially, try to add a componant yourself like the holidays"]}]}